{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss9WR_zbQELO"
   },
   "source": [
    "# Practice 4: Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB3vM2GfQELQ"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "### Formulation of the problem\n",
    "\n",
    "In this assignment, you will solve the Named Entity Recognition (NER) problem, one of the most common in NLP, along with the text classification problem.\n",
    "\n",
    "This task involves classifying each word/token whether it is part of a named entity (an entity may consist of multiple words/tokens) or not.\n",
    "\n",
    "For example, we want to extract names and organization names. Then for the text\n",
    "\n",
    "     Yan    Goodfellow  works  for  Google  Brain\n",
    "\n",
    "The model should extract the following sequence:\n",
    "\n",
    "     B-PER  I-PER       O      O    B-ORG   I-ORG\n",
    "\n",
    "where the prefixes *B-* and *I-* denote the beginning and end of the named entity, *O* denotes a word without a tag. This prefix system (*BIO* tagging) was introduced to distinguish between successive named entities of the same type.\n",
    "There are other types of tagging, such as [*BILUO*](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)), but for this tutorial we will focus on *BIO*.\n",
    "\n",
    "We will solve the NER problem on the CoNLL-2003 dataset using recurrent networks and models based on the Transformer architecture.\n",
    "\n",
    "### Libraries\n",
    "\n",
    "Main libraries:\n",
    "  - [PyTorch](https://pytorch.org/)\n",
    "  - [Transformers](https://github.com/huggingface/transformers)\n",
    "\n",
    "### Data\n",
    "\n",
    "The data is stored in an archive, which consists of:\n",
    "\n",
    "- *train.tsv* - training sample. Each line contains: <word / token>, <word / token tag>\n",
    "\n",
    "- *valid.tsv* - validation sample, which can be used to select hyperparameters and quality measurements. It has an identical structure to train.tsv.\n",
    "\n",
    "- *test.tsv* - test sample, which is used to evaluate the final quality. It has an identical structure to train.tsv.\n",
    "\n",
    "You can download the data here: [link](https://drive.google.com/drive/folders/1OKNrfHsBm1ehbG-yM0R1BGshbscf_eue?usp=drive_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5BCB1EfQan1"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy==1.21.6 scikit-learn==1.0.2 torch==1.12.1 tqdm==4.64.0 transformers==4.21.1 wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Thidpb9qQELS"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiDlmbY2QELT"
   },
   "source": [
    "Let's fix the seed for reproducibility of the results (it is advisable to do this **always**!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yt3ISg3aQELU"
   },
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set global seed for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhIg0ZBzQELV"
   },
   "source": [
    "Let’s initialize the device (CPU / GPU) on which we will work (preferably **GPU**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rboLOv95QELV"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UW16ryFIQELW"
   },
   "source": [
    "Initialize *tensorboard* to log metrics during the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6O7Y8hReTODp"
   },
   "outputs": [],
   "source": [
    "# Initialize wandb for logging\n",
    "wandb.init(\n",
    "    project=\"ner-homework\",\n",
    "    name=\"ner-experiment\",\n",
    "    config={\n",
    "        \"task\": \"named_entity_recognition\",\n",
    "        \"dataset\": \"conll2003\",\n",
    "        \"framework\": \"pytorch\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k3Nhd3IQELY"
   },
   "source": [
    "## Part 1. Data preparation (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qYjOMuPQELY"
   },
   "source": [
    "First of all, we need to read the data. Let's write a function that takes as input the path to one of the conll-2003 files and returns two lists:\n",
    "- a list of lists of words/tokens (and corresponding to it)\n",
    "- list of lists of tags\n",
    "\n",
    "P.S. Let's make this function more flexible by supplying a boolean variable as input, whether we read data in *lowercase* or not.\n",
    "\n",
    "**Exercise. Implement the `read_conll2003` function.** **<font color='red'>(1 point)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQdCfX2OQELZ"
   },
   "outputs": [],
   "source": [
    "def read_conll2003(\n",
    "    path: str,\n",
    "    lower: bool = True,\n",
    ") -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Prepare data in CoNNL like format.\n",
    "    \"\"\"\n",
    "\n",
    "    token_seq = []\n",
    "    label_seq = []\n",
    "\n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Empty line indicates end of sentence\n",
    "            if not line:\n",
    "                if current_tokens:  # Only add if we have tokens\n",
    "                    token_seq.append(current_tokens)\n",
    "                    label_seq.append(current_labels)\n",
    "                    current_tokens = []\n",
    "                    current_labels = []\n",
    "            else:\n",
    "                # Split by tab to get token and label\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    token = parts[0]\n",
    "                    label = parts[1]\n",
    "                    \n",
    "                    if lower:\n",
    "                        token = token.lower()\n",
    "                    \n",
    "                    current_tokens.append(token)\n",
    "                    current_labels.append(label)\n",
    "    \n",
    "    # Don't forget the last sentence if file doesn't end with empty line\n",
    "    if current_tokens:\n",
    "        token_seq.append(current_tokens)\n",
    "        label_seq.append(current_labels)\n",
    "\n",
    "    return token_seq, label_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYm8xEvFQELb"
   },
   "source": [
    "Let's read all three files:\n",
    "\n",
    "- *train.tsv*\n",
    "- *valid.tsv*\n",
    "- *test.tsv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-inr1BPgQELb"
   },
   "outputs": [],
   "source": [
    "train_token_seq, train_label_seq = read_conll2003(\"train.txt\")\n",
    "valid_token_seq, valid_label_seq = read_conll2003(\"valid.txt\")\n",
    "test_token_seq, test_label_seq = read_conll2003(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the data files exist\n",
    "import os\n",
    "data_files = [\"train.txt\", \"valid.txt\", \"test.txt\"]\n",
    "for file in data_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"✓ {file} found\")\n",
    "    else:\n",
    "        print(f\"✗ {file} not found\")\n",
    "        \n",
    "if all(os.path.exists(f) for f in data_files):\n",
    "    print(\"All data files are ready!\")\n",
    "else:\n",
    "    print(\"Please make sure all .txt files are in the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOoNc1VUQELc"
   },
   "source": [
    "Look at what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HK8AcwWGQELd"
   },
   "outputs": [],
   "source": [
    "for token, label in zip(train_token_seq[0], train_label_seq[0]):\n",
    "    print(f\"{token}\\t{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8SqDeMJjF3Y"
   },
   "outputs": [],
   "source": [
    "for token, label in zip(valid_token_seq[0], valid_label_seq[0]):\n",
    "    print(f\"{token}\\t{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddFE7p5kjF_p"
   },
   "outputs": [],
   "source": [
    "for token, label in zip(test_token_seq[0], test_label_seq[0]):\n",
    "    print(f\"{token}\\t{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZ4Go3IXfDit"
   },
   "outputs": [],
   "source": [
    "assert len(train_token_seq) == len(train_label_seq), \"The lengths of the training token_seq and label_seq do not match, an error in the read_conll2003 function\"\n",
    "assert len(valid_token_seq) == len(valid_label_seq), \"The lengths of the validation token_seq and label_seq do not match, an error in the read_conll2003 function\"\n",
    "assert len(test_token_seq) == len(test_label_seq), \"The lengths of the test token_seq and label_seq do not match, an error in the read_conll2003 function\"\n",
    "\n",
    "assert train_token_seq[0] == ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'], \"Error in training token_seq\"\n",
    "assert train_label_seq[0] == ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], \"Error in training label_seq\"\n",
    "\n",
    "assert valid_token_seq[0] == ['cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top', 'after', 'innings', 'victory', '.'], \"Error in validation token_seq\"\n",
    "assert valid_label_seq[0] == ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], \"Error in validation label_seq\"\n",
    "\n",
    "assert test_token_seq[0] == ['soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.'], \"Error in test token_seq\"\n",
    "assert test_label_seq[0] == ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O'], \"Error in test label_seq\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j96zKo6PQELd"
   },
   "source": [
    "The CoNLL-2003 dataset is presented in the form of **BIO** tagging, where the label is:\n",
    "- *B-{label}* - beginning of entity *{label}*\n",
    "- *I-{label}* - continuation of the entity *{label}*\n",
    "- *O* - no entity\n",
    "\n",
    "There are also other sequence tagging methods, such as **BILUO**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SVL4USbQELe"
   },
   "source": [
    "### Preparing dictionaries\n",
    "\n",
    "To train the neural network, we will use two mappings:\n",
    "- {**token**}→{**token_idx**}: correspondence between word/token and string in *embedding* matrix (starts from 0);\n",
    "- {**label**}→{**label_idx**}: correspondence between tag and unique index (starts from 0);\n",
    "\n",
    "Now we need to implement two functions:\n",
    "- get_token2idx\n",
    "- get_label2idx\n",
    "\n",
    "which will return the corresponding dictionaries.\n",
    "\n",
    "P.S. token2idx dictionary must also contain special tokens:\n",
    "- `<PAD>` is a special token for padding, since we are going to train the models in batches\n",
    "- `<UNK>` is a special token for processing words/tokens that are not in the dictionary (relevant for inference)\n",
    "\n",
    "Let's assign them to idx 0 and 1 respectively for convenience.\n",
    "\n",
    "P.P.S. You can also add a *min_count* parameter to get_token2idx, which will only include words exceeding a certain frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOnc3UHpQELf"
   },
   "source": [
    "First let's collect:\n",
    "- token2cnt - a dictionary from a unique word / token to the number of these words / tokens in the training set (it is important that only in the training set!)\n",
    "- label_set - a list of unique tags\n",
    "\n",
    "P.S. You can also use stemming to convert different word forms of the same word into one token, but we will skip this point.\n",
    "\n",
    "**Exercise. Implement the `get_token2idx` and `get_label2idx` functions.** **<font color='red'>(1 point)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IthnXKsoo7A3"
   },
   "outputs": [],
   "source": [
    "token2cnt = Counter([token for sentence in train_token_seq for token in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_v8YUM7QELg"
   },
   "outputs": [],
   "source": [
    "token2cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSm7B546nmDh"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of unique words in the training dataset: {len(token2cnt)}\")\n",
    "print(f\"Number of words occurring only once in the training dataset: {len([token for token, cnt in token2cnt.items() if cnt == 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRtCHt1QruSU"
   },
   "source": [
    "As we can see, we have many words that appear only once in the dataset. Obviously, we won’t be able to learn from them, we will only overfit, so let’s throw out such words when forming our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCaPftCyQELi"
   },
   "outputs": [],
   "source": [
    "# use the min_count parameter to cut off words with frequency cnt < min_count\n",
    "\n",
    "def get_token2idx(\n",
    "    token2cnt: Dict[str, int],\n",
    "    min_count: int,\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get mapping from tokens to indices to use with Embedding layer.\n",
    "    \"\"\"\n",
    "\n",
    "    token2idx: Dict[str, int] = {}\n",
    "\n",
    "    # Add special tokens first\n",
    "    token2idx[\"<PAD>\"] = 0\n",
    "    token2idx[\"<UNK>\"] = 1\n",
    "    \n",
    "    # Add tokens that meet min_count requirement\n",
    "    idx = 2\n",
    "    for token, count in token2cnt.items():\n",
    "        if count >= min_count:\n",
    "            token2idx[token] = idx\n",
    "            idx += 1\n",
    "\n",
    "    return token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFK130y-sLH4"
   },
   "outputs": [],
   "source": [
    "token2idx = get_token2idx(token2cnt, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g69HFZC7QELh"
   },
   "outputs": [],
   "source": [
    "# Function for sorting tags so that first there is an O tag,\n",
    "# then B- tags and only after I- tags (can be set manually)\n",
    "\n",
    "def sort_labels_func(x: str) -> int:\n",
    "    if x == \"O\":\n",
    "        return 0\n",
    "    elif x.startswith(\"B-\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "label_set = sorted(\n",
    "    set(label for sentence in train_label_seq for label in sentence),\n",
    "    key=lambda x: (sort_labels_func(x), x),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VI_3m4qbQELi"
   },
   "outputs": [],
   "source": [
    "label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6i51GPtQELj"
   },
   "outputs": [],
   "source": [
    "def get_label2idx(label_set: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get mapping from labels to indices.\n",
    "    \"\"\"\n",
    "\n",
    "    label2idx: Dict[str, int] = {}\n",
    "\n",
    "    for idx, label in enumerate(label_set):\n",
    "        label2idx[label] = idx\n",
    "\n",
    "    return label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XW6fK0HtQELk"
   },
   "outputs": [],
   "source": [
    "label2idx = get_label2idx(label_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U13l-2IOQELk"
   },
   "source": [
    "Let's look at what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7U7bMrHQELl"
   },
   "outputs": [],
   "source": [
    "for token, idx in list(token2idx.items())[:10]:\n",
    "    print(f\"{token}\\t{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hp75V-o2QELl"
   },
   "outputs": [],
   "source": [
    "for label, idx in label2idx.items():\n",
    "    print(f\"{label}\\t{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYb4BdAUhNzk"
   },
   "outputs": [],
   "source": [
    "assert len(get_token2idx(token2cnt, min_count=1)) == 21012, \"Error in dictionary length, most likely min_count is implemented incorrectly\"\n",
    "assert len(token2idx) == 10952, \"Incorrect token2idx length, most likely min_count is implemented incorrectly\"\n",
    "assert len(label2idx) == 9, \"Incorrect label2idx length\"\n",
    "\n",
    "assert list(token2idx.items())[:10] == [\n",
    "    ('<PAD>', 0), ('<UNK>', 1), ('eu', 2), ('german', 3), ('call', 4),\n",
    "    ('to', 5), ('boycott', 6), ('british', 7), ('lamb', 8), ('.', 9)\n",
    "], \"Wrong format of token2idx\"\n",
    "assert label2idx == {\n",
    "    'O': 0, 'B-LOC': 1, 'B-MISC': 2, 'B-ORG': 3, 'B-PER': 4,\n",
    "    'I-LOC': 5, 'I-MISC': 6, 'I-ORG': 7, 'I-PER': 8\n",
    "}, \"Wrong format of label2idx\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItPs1DmOQELm"
   },
   "source": [
    "### Preparing the dataset and loader\n",
    "\n",
    "Typically, neural networks are trained in batches. This means that each update of the neural network's weights occurs based on multiple sequences. A technical detail is the need to complete all sequences within the batch to the same length.\n",
    "\n",
    "From the previous practical task, you should know about `Dataset` (`torch.utils.data.Dataset`) - a data structure that stores and can index data for training. The dataset must inherit from the standard PyTorch Dataset class and override the `__len__` and `__getitem__` methods.\n",
    "\n",
    "The `__getitem__` method must return the indexed sequence and its tags.\n",
    "\n",
    "**Don't forget** about `<UNK>` special token for unknown words!\n",
    "\n",
    "Let's write a custom dataset for our task, which will receive as input (the `__init__` method):\n",
    "- token_seq - list of lists of words/tokens\n",
    "- label_seq - list of lists of tags\n",
    "- token2idx\n",
    "- label2idx\n",
    "\n",
    "and return from the `__getitem__` method two int64 tensors (`torch.LongTensor`) with the indices of words / tokens in the sample and the indices of the corresponding tags:\n",
    "\n",
    "**Exercise. Implement the NERDataset class.** **<font color='red'>(1 point)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdZvnUUpQELm"
   },
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for NER.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_seq: List[List[str]],\n",
    "        label_seq: List[List[str]],\n",
    "        token2idx: Dict[str, int],\n",
    "        label2idx: Dict[str, int],\n",
    "    ):\n",
    "        self.token2idx = token2idx\n",
    "        self.label2idx = label2idx\n",
    "\n",
    "        self.token_seq = [self.process_tokens(tokens, token2idx) for tokens in token_seq]\n",
    "        self.label_seq = [self.process_labels(labels, label2idx) for labels in label_seq]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_seq)\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        return torch.LongTensor(self.token_seq[idx]), torch.LongTensor(self.label_seq[idx])\n",
    "\n",
    "    @staticmethod\n",
    "    def process_tokens(\n",
    "        tokens: List[str],\n",
    "        token2idx: Dict[str, int],\n",
    "        unk: str = \"<UNK>\",\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Transform list of tokens into list of tokens' indices.\n",
    "        \"\"\"\n",
    "        token_indices = []\n",
    "        for token in tokens:\n",
    "            if token in token2idx:\n",
    "                token_indices.append(token2idx[token])\n",
    "            else:\n",
    "                token_indices.append(token2idx[unk])\n",
    "        return token_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def process_labels(\n",
    "        labels: List[str],\n",
    "        label2idx: Dict[str, int],\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Transform list of labels into list of labels' indices.\n",
    "        \"\"\"\n",
    "        return [label2idx[label] for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCvaPJERQELn"
   },
   "source": [
    "Create three datasets:\n",
    "- *train_dataset*\n",
    "- *valid_dataset*\n",
    "- *test_dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUMsSNkoQELn"
   },
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(\n",
    "    token_seq=train_token_seq,\n",
    "    label_seq=train_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "valid_dataset = NERDataset(\n",
    "    token_seq=valid_token_seq,\n",
    "    label_seq=valid_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "test_dataset = NERDataset(\n",
    "    token_seq=test_token_seq,\n",
    "    label_seq=test_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQIq1pAWQELo"
   },
   "source": [
    "Let's look at what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_Scync0QELo"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyAazaLzjQ-K"
   },
   "outputs": [],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHuuh3YmjRNt"
   },
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gox6uyF2idwZ"
   },
   "outputs": [],
   "source": [
    "assert len(train_dataset) == 14986, \"Incorrect train_dataset length\"\n",
    "assert len(valid_dataset) == 3465, \"Incorrect valid_dataset length\"\n",
    "assert len(test_dataset) == 3683, \"Incorrect test_dataset length\"\n",
    "\n",
    "assert torch.equal(train_dataset[0][0], torch.tensor([2,1,3,4,5,6,7,8,9])), \"Malformed train_dataset\"\n",
    "assert torch.equal(train_dataset[0][1], torch.tensor([3,0,2,0,0,0,2,0,0])), \"Malformed train_dataset\"\n",
    "\n",
    "assert torch.equal(\n",
    "    valid_dataset[0][0],\n",
    "    torch.tensor([1737,571,1777,197,687,145,349,111,1819,1558,9])\n",
    "), \"Malformed valid_dataset\"\n",
    "assert torch.equal(valid_dataset[0][1], torch.tensor([0,0,3,0,0,0,0,0,0,0,0])), \"Malformed valid_dataset\"\n",
    "\n",
    "assert torch.equal(\n",
    "    test_dataset[0][0],\n",
    "    torch.tensor([1516,571,1434,1729,4893,2014,67,310,215,3157,3139,9])\n",
    "), \"Malformed test_dataset\"\n",
    "assert torch.equal(test_dataset[0][1], torch.tensor([0,0,1,0,0,0,0,4,0,0,0,0])), \"Malformed test_dataset\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWjJuAk7QELp"
   },
   "source": [
    "In order to complete sequences with padding, we will use the `collate_fn` parameter of the `DataLoader` class.\n",
    "\n",
    "Given a sequence of pairs of tensors for sentences and tags, it is necessary to complete all sequences to the sequence of the maximum length in the batch.\n",
    "\n",
    "Use the special token `<PAD>` for completion of word/token sequences and -1 for tag sequences.\n",
    "\n",
    "**hint**: it is convenient to use the `torch.nn.utils.rnn` method. Pay attention to the `batch_first` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZiJVM5qQELp"
   },
   "source": [
    "`Collator` can be implemented in two ways:\n",
    "- class with method `__call__`\n",
    "- function\n",
    "\n",
    "We will go the first way.\n",
    "\n",
    "Initialize an instance of the `Collator` class (the `__init__` method) using two parameters:\n",
    "- id `<PAD>` special token for word/token sequences\n",
    "- id `<PAD>` special token for tag sequences (value -1)\n",
    "\n",
    "The `__call__` method takes a batch as input, namely a list of tuples of what is returned from the `__getitem__` method of our dataset. In our case, this is a list of tuples of two int64 tensors - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
    "\n",
    "Ad the output we want to get two tensors:\n",
    "- Indexes of word/token with paddings\n",
    "- Indexes of tags with paddings\n",
    "    \n",
    "P.S. The `<PAD>` value is needed to easily distinguish pad tokens from others when calculating loss. You can use the `ignore_index` parameter when initializing the loss.\n",
    "\n",
    "**Exercise. Implement the collator class NERCollator.** **<font color='red'>(1 point)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNHNwoLnQELp"
   },
   "outputs": [],
   "source": [
    "class NERCollator:\n",
    "    \"\"\"\n",
    "    Collator that handles variable-size sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_padding_value: int,\n",
    "        label_padding_value: int,\n",
    "    ):\n",
    "        self.token_padding_value = token_padding_value\n",
    "        self.label_padding_value = label_padding_value\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: List[Tuple[torch.LongTensor, torch.LongTensor]],\n",
    "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "\n",
    "        tokens, labels = zip(*batch)\n",
    "\n",
    "        # Pad sequences using PyTorch utility\n",
    "        padded_tokens = torch.nn.utils.rnn.pad_sequence(\n",
    "            tokens, \n",
    "            batch_first=True, \n",
    "            padding_value=self.token_padding_value\n",
    "        )\n",
    "        \n",
    "        padded_labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, \n",
    "            batch_first=True, \n",
    "            padding_value=self.label_padding_value\n",
    "        )\n",
    "\n",
    "        return padded_tokens, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZUMwVQTQELq"
   },
   "outputs": [],
   "source": [
    "collator = NERCollator(\n",
    "    token_padding_value=token2idx[\"<PAD>\"],\n",
    "    label_padding_value=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jsgfij8WQELq"
   },
   "source": [
    "Now everything is ready to define the loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFljkiBOQELr"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,  # for correct metrics measurements leave batch_size=1\n",
    "    shuffle=False, # for correct metrics measurements leave shuffle=False\n",
    "    collate_fn=collator,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # for correct metrics measurements leave batch_size=1\n",
    "    shuffle=False, # for correct metrics measurements leave shuffle=False\n",
    "    collate_fn=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i34wGJ4uQELr"
   },
   "source": [
    "Let's look at what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLlr_DztQELr"
   },
   "outputs": [],
   "source": [
    "tokens, labels = next(iter(train_dataloader))\n",
    "\n",
    "tokens = tokens.to(device)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdMMEDdbQELs"
   },
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w--fhADKQELs"
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFeX0AYKlhGk"
   },
   "outputs": [],
   "source": [
    "train_tokens, train_labels = next(iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "))\n",
    "assert torch.equal(\n",
    "    train_tokens,\n",
    "    torch.tensor([[2, 1, 3, 4, 5, 6, 7, 8, 9], [10, 11, 0, 0, 0, 0, 0, 0, 0]])\n",
    "), \"Looks like a bug in the collator\"\n",
    "assert torch.equal(\n",
    "    train_labels,\n",
    "    torch.tensor([[3, 0, 2, 0, 0, 0, 2, 0, 0], [4, 8, -1, -1, -1, -1, -1, -1, -1]])\n",
    "), \"Looks like a bug in the collator\"\n",
    "\n",
    "valid_tokens, valid_labels = next(iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "))\n",
    "assert torch.equal(\n",
    "    valid_tokens,\n",
    "    torch.tensor([\n",
    "        [1737, 571, 1777, 197, 687, 145, 349, 111,  1819, 1558, 9],\n",
    "        [248, 10679, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ])), \"Looks like a bug in the collator\"\n",
    "assert torch.equal(\n",
    "    valid_labels,\n",
    "    torch.tensor([\n",
    "        [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "    ])), \"Looks like a bug in the collator\"\n",
    "\n",
    "test_tokens, test_labels = next(iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "))\n",
    "assert torch.equal(\n",
    "    test_tokens,\n",
    "    torch.tensor([\n",
    "        [1516, 571, 1434, 1729, 4893, 2014, 67, 310, 215, 3157, 3139, 9],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ])), \"Looks like a bug in the collator\"\n",
    "assert torch.equal(\n",
    "    test_labels,\n",
    "    torch.tensor([\n",
    "        [0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0],\n",
    "        [4, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "    ])), \"Looks like a bug in the collator\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ul5gLriQELs"
   },
   "source": [
    "## Part 2. BiLSTM tagger (6 points)\n",
    "\n",
    "Define the network architecture using the PyTorch library.\n",
    "\n",
    "Your architecture at this point should follow the standard tagger:\n",
    "* Embedding layer at the input\n",
    "* LSTM (unidirectional or bidirectional) layer for sequence processing\n",
    "* Dropout (specified separately or built into LSTM) to reduce overfitting\n",
    "* Linear output layer\n",
    "\n",
    "To train the network, use an element-wise cross-entropy loss function.\n",
    "\n",
    "**Please note** that `<PAD>` tokens should not be included in the loss function calculation. It is recommended to use Adam as an optimizer. To obtain prediction values from model outputs, use the `argmax` function.\n",
    "\n",
    "**Exercise. Implement the BiLSTM model class.** **<font color='red'>(2 points)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMiLQljZQELt"
   },
   "outputs": [],
   "source": [
    "class BiLSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        bidirectional: bool,\n",
    "        n_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Calculate input size for the head\n",
    "        rnn_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        self.head = torch.nn.Linear(rnn_output_size, n_classes)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tokens: torch.LongTensor) -> torch.Tensor:\n",
    "        embed = self.embedding(tokens)\n",
    "\n",
    "        # we use the special function pack_padded_sequence in order to obtain a PackedSequence structure\n",
    "        # that does not take padding into account when passing rnn\n",
    "        length = (tokens != 0).sum(dim=1).detach().cpu()\n",
    "        packed_embed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embed, length, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # we use the special function pad_packed_sequence to get a tensor from PackedSequence\n",
    "        packed_rnn_output, _ = self.rnn(packed_embed)\n",
    "        rnn_output, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_rnn_output, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Apply dropout\n",
    "        rnn_output = self.dropout(rnn_output)\n",
    "        \n",
    "        logits = self.head(rnn_output)\n",
    "        return logits.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zps8HL2VQELu"
   },
   "outputs": [],
   "source": [
    "model = BiLSTM(\n",
    "    num_embeddings=len(token2idx),\n",
    "    embedding_dim=100,\n",
    "    hidden_size=100,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    bidirectional=True,\n",
    "    n_classes=len(label2idx),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmg2_C_oQELu"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDvWB5J2QELv"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jn5Pu1UKQELv"
   },
   "outputs": [],
   "source": [
    "outputs = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n02Bsh8eQELw"
   },
   "outputs": [],
   "source": [
    "assert outputs.shape == torch.Size([2, 9, 10])\n",
    "assert 2 < criterion(outputs, labels) < 3\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjhkK9QFQELu"
   },
   "source": [
    "### Experiments\n",
    "\n",
    "Run experiments on the data. Adjust parameters based on the validation set without using the test set. Your goal is to configure the network so that the quality of the model according to the F1-macro measure on the validation and test sets is no less than **0.76**.\n",
    "\n",
    "Draw conclusions about model quality, overfitting, and sensitivity of the architecture to the choice of hyperparameters. Present the results of your experiments in the form of a mini-report (in the same ipython notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4hdrFZ9iRPi"
   },
   "outputs": [],
   "source": [
    "# We'll use wandb for logging instead of TensorBoard\n",
    "print(\"Using Weights & Biases for experiment tracking and visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruMTBSkQQELx"
   },
   "source": [
    "**Exercise. Implement a metric calculation function `compute_metrics`.** **<font color='red'>(1 point)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkpo3JgWQELx"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(\n",
    "    outputs: torch.Tensor,\n",
    "    labels: torch.LongTensor,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute NER metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Convert outputs to predictions\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    # Flatten and convert to numpy\n",
    "    predictions = predictions.view(-1).cpu().numpy()\n",
    "    labels = labels.view(-1).cpu().numpy()\n",
    "    \n",
    "    # Filter out padding tokens (labels = -1)\n",
    "    mask = labels != -1\n",
    "    y_true = labels[mask]\n",
    "    y_pred = predictions[mask]\n",
    "\n",
    "    # accuracy\n",
    "    accuracy = accuracy_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "    )\n",
    "\n",
    "    # precision\n",
    "    precision_micro = precision_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"micro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    precision_macro = precision_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"macro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    precision_weighted = precision_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"weighted\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    # recall\n",
    "    recall_micro = recall_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"micro\",\n",
    "        zero_division=0,\n",
    "\n",
    "    )\n",
    "    recall_macro = recall_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"macro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    recall_weighted = recall_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"weighted\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    # f1\n",
    "    f1_micro = f1_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"micro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    f1_macro = f1_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"macro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    f1_weighted = f1_score(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        average=\"weighted\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    metrics[\"accuracy\"] = accuracy\n",
    "\n",
    "    metrics[\"precision_micro\"]    = precision_micro\n",
    "    metrics[\"precision_macro\"]    = precision_macro\n",
    "    metrics[\"precision_weighted\"] = precision_weighted\n",
    "\n",
    "    metrics[\"recall_micro\"]    = recall_micro\n",
    "    metrics[\"recall_macro\"]    = recall_macro\n",
    "    metrics[\"recall_weighted\"] = recall_weighted\n",
    "\n",
    "    metrics[\"f1_micro\"]    = f1_micro\n",
    "    metrics[\"f1_macro\"]    = f1_macro\n",
    "    metrics[\"f1_weighted\"] = f1_weighted\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzj89UygQEL0"
   },
   "source": [
    "**Exercise. Implement the training and testing functions `train_epoch` and `evaluate_epoch`. <font color='red'>(2 points)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG3vQbc_QEL0"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    One training cycle (loop).\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    for i, (tokens, labels) in tqdm(\n",
    "        enumerate(dataloader),\n",
    "        total=len(dataloader),\n",
    "        desc=\"loop over train batches\",\n",
    "    ):\n",
    "\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "        # Loss calculation and optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(tokens)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        # Log to wandb every 100 batches\n",
    "        if i % 100 == 0:\n",
    "            wandb.log({\n",
    "                \"train/batch_loss\": loss.item(),\n",
    "                \"train/epoch\": epoch,\n",
    "                \"train/batch\": epoch * len(dataloader) + i\n",
    "            })\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs_inference = model(tokens)\n",
    "            model.train()\n",
    "\n",
    "        batch_metrics = compute_metrics(\n",
    "            outputs=outputs_inference,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        for metric_name, metric_value in batch_metrics.items():\n",
    "            batch_metrics_list[metric_name].append(metric_value)\n",
    "\n",
    "    avg_loss = np.mean(epoch_loss)\n",
    "    print(f\"Train loss: {avg_loss}\\n\")\n",
    "    \n",
    "    # Log epoch metrics to wandb\n",
    "    wandb_log = {\"train/loss\": avg_loss, \"epoch\": epoch}\n",
    "    \n",
    "    for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "        metric_value = np.mean(metric_value_list)\n",
    "        print(f\"Train {metric_name}: {metric_value}\\n\")\n",
    "        wandb_log[f\"train/{metric_name}\"] = metric_value\n",
    "    \n",
    "    wandb.log(wandb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4ztFogtQEL0"
   },
   "outputs": [],
   "source": [
    "def evaluate_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    split_name: str = \"val\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    One evaluation cycle (loop).\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (tokens, labels) in tqdm(\n",
    "            enumerate(dataloader),\n",
    "            total=len(dataloader),\n",
    "            desc=f\"loop over {split_name} batches\",\n",
    "        ):\n",
    "\n",
    "            tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "            # Loss calculation\n",
    "            outputs = model(tokens)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            batch_metrics = compute_metrics(\n",
    "                outputs=outputs,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            for metric_name, metric_value in batch_metrics.items():\n",
    "                batch_metrics_list[metric_name].append(metric_value)\n",
    "\n",
    "        avg_loss = np.mean(epoch_loss)\n",
    "        print(f\"{split_name.capitalize()} loss:  {avg_loss}\\n\")\n",
    "        \n",
    "        # Log epoch metrics to wandb\n",
    "        wandb_log = {f\"{split_name}/loss\": avg_loss, \"epoch\": epoch}\n",
    "\n",
    "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "            metric_value = np.mean(metric_value_list)\n",
    "            print(f\"{split_name.capitalize()} {metric_name}: {metric_value}\\n\")\n",
    "            wandb_log[f\"{split_name}/{metric_name}\"] = metric_value\n",
    "        \n",
    "        wandb.log(wandb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7Z5MTNzQEL1"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    n_epochs: int,\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
    "\n",
    "        train_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "        evaluate_epoch(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "            split_name=\"val\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTxfU0BfQEL1"
   },
   "source": [
    "**Exercise. Conduct experiments. <font color='red'>(2 points)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yz6mjGZUQEL2"
   },
   "outputs": [],
   "source": [
    "# Experiment with BiLSTM\n",
    "wandb.config.update({\n",
    "    \"model_type\": \"BiLSTM\",\n",
    "    \"embedding_dim\": 100,\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"bidirectional\": True,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 5\n",
    "})\n",
    "\n",
    "model = BiLSTM(\n",
    "    num_embeddings=len(token2idx),\n",
    "    embedding_dim=100,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    bidirectional=True,\n",
    "    n_classes=len(label2idx),\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "print(\"Starting BiLSTM experiment...\")\n",
    "print(f\"Model parameters: embedding_dim=100, hidden_size=128, num_layers=2, dropout=0.1, lr=1e-3\")\n",
    "\n",
    "train(\n",
    "    n_epochs=5,\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=valid_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Test final model on test set\n",
    "print(\"Final evaluation on test set:\")\n",
    "evaluate_epoch(\n",
    "    model=model,\n",
    "    dataloader=test_dataloader,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    epoch=0,\n",
    "    split_name=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8R6nopyQEL-"
   },
   "source": [
    "## Part 3. Transformers tagger (6 points)\n",
    "\n",
    "In this part of the task, you need to do the same thing, but using a model based on the Transformer architecture, namely, it is proposed to additionally fine-tune the pre-trained **BERT** model.\n",
    "\n",
    "This model requires special data preparation, which is where we will start:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrbX5gFDQEL-"
   },
   "source": [
    "The **BERT** model uses a custom WordPiece tokenizer to break sentences into tokens. A pre-trained version of such a tokenizer exists in the `transformers` library. There are two classes: `BertTokenizer` and `BertTokenizerFast`. You can use either one, but the second option works much faster because it is written in C programming language.\n",
    "\n",
    "Tokenizers can be trained from scratch using your own data corpus, or you can load pre-trained ones. Pre-trained tokenizers typically match a pre-trained model configuration that uses the vocabulary from that tokenizer.\n",
    "\n",
    "We will use a basic pretrained **BERT** configuration for the model and tokenizer.\n",
    "\n",
    "P.S. Often you have to experiment with models of different architectures, for example **BERT** and **GPT**, so it is convenient to use the `AutoTokenizer` class, which, based on the name of the model, will determine which class is needed to initialize the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-UTiI4gQEL-"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSbBhvnDQEMA"
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxWNX5i6QEMA"
   },
   "source": [
    "Pretrained models and tokenizers are loaded from `huggingface` using the `from_pretrained` constructor.\n",
    "\n",
    "In this constructor, you can specify either the path to the pretrained tokenizer, or the name of the pretrained configuration, as in our case. `transformers` will load the necessary parameters itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tg_bCeaQEMA"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MIrbmNoQEMA"
   },
   "source": [
    "### Preparing dictionaries\n",
    "\n",
    "Compared to recurrent models, there is no more need to build a dictionary, since this is already done in advance thanks to tokenizers and the algorithms behind them.\n",
    "\n",
    "But as before, we will need:\n",
    "- {**label**}→{**label_idx**}: correspondence between tag and unique index (starts from 0);\n",
    "\n",
    "We have already implemented this mapping in one of the previous parts of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvYF-4uaQEMB"
   },
   "source": [
    "### Preparing the dataset and loader\n",
    "\n",
    "We also want to train the model in batches, so we will still need `Dataset`, `Collator` and `DataLoader`.\n",
    "\n",
    "But we cannot reuse those from the previous parts of the task, since the data processing must be done a little differently using a tokenizer.\n",
    "\n",
    "Let's write a new custom dataset that will receive as input (the `__init__` method):\n",
    "- token_seq - list of lists of words/tokens\n",
    "- label_seq - list of lists of tags\n",
    "\n",
    "and return two lists from the `__getitem__` method:\n",
    "- list of text values (`List[str]`) from token indices in the sample\n",
    "- a list of integer values (`List[int]`) from the indices of the corresponding tags\n",
    "\n",
    "P.S. Unlike the previous custom dataset, here we return two `Lists` instead of `torch.LongTensor`, since we will transfer the logic for generating a padded batch to `Collator` due to the specifics of the tokenizer - it itself returns an already padded tensor with token indexes, and for tag indexes we will need to do this ourselves, similar to the previous dataset.\n",
    "\n",
    "**Exercise. Implement the TransformersDataset class. <font color='red'>(1 point)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EoNLDOOQEMB"
   },
   "outputs": [],
   "source": [
    "class TransformersDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Transformers Dataset for NER.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_seq: List[List[str]],\n",
    "        label_seq: List[List[str]],\n",
    "    ):\n",
    "        self.token_seq = token_seq\n",
    "        self.label_seq = [self.process_labels(labels, label2idx) for labels in label_seq]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_seq)\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "    ) -> Tuple[List[str], List[int]]:\n",
    "        return self.token_seq[idx], self.label_seq[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def process_labels(\n",
    "        labels: List[str],\n",
    "        label2idx: Dict[str, int],\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Transform list of labels into list of labels' indices.\n",
    "        \"\"\"\n",
    "        return [label2idx[label] for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1oNc-31QEMB"
   },
   "source": [
    "Create three datasets:\n",
    "- *train_dataset*\n",
    "- *valid_dataset*\n",
    "- *test_dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqg56Jf8QEMC"
   },
   "outputs": [],
   "source": [
    "train_dataset = TransformersDataset(\n",
    "    token_seq=train_token_seq,\n",
    "    label_seq=train_label_seq,\n",
    ")\n",
    "valid_dataset = TransformersDataset(\n",
    "    token_seq=valid_token_seq,\n",
    "    label_seq=valid_label_seq,\n",
    ")\n",
    "test_dataset = TransformersDataset(\n",
    "    token_seq=test_token_seq,\n",
    "    label_seq=test_label_seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdIS6XrvQEMC"
   },
   "source": [
    "Let's look at what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IT00Pjy6QEMC"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYal2icQmuD-"
   },
   "outputs": [],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCXd3FWVmuKe"
   },
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4R605vAnYT9"
   },
   "outputs": [],
   "source": [
    "assert len(train_dataset) == 14986, \"Incorrect train_dataset length\"\n",
    "assert len(valid_dataset) == 3465, \"Incorrect valid_dataset length\"\n",
    "assert len(test_dataset) == 3683, \"Incorrect test_dataset length\"\n",
    "\n",
    "assert train_dataset[0][0] == ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'], \"Malformed train_dataset\"\n",
    "assert train_dataset[0][1] == [3,0,2,0,0,0,2,0,0], \"Malformed train_dataset\"\n",
    "\n",
    "assert valid_dataset[0][0] == ['cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top', 'after', 'innings', 'victory', '.'], \"Malformed valid_dataset\"\n",
    "assert valid_dataset[0][1] == [0,0,3,0,0,0,0,0,0,0,0], \"Malformed valid_dataset\"\n",
    "\n",
    "assert test_dataset[0][0] == ['soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.'], \"Malformed test_dataset\"\n",
    "assert test_dataset[0][1] == [0,0,1,0,0,0,0,4,0,0,0,0], \"Malformed test_dataset\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zP_6iQnQEMC"
   },
   "source": [
    "Let's implement a new `Collator`.\n",
    "\n",
    "The collator will be initialized with 3 arguments:\n",
    "- tokenizer\n",
    "- tokenizer parameters in the form of a dictionary (then used as `**kwargs`)\n",
    "- special token id for tag sequences (value -1)\n",
    "\n",
    "The `__call__` method takes a batch as input, namely a list of tuples of what is returned from the dataset with `__getitem__` method. In our case, this is a list of tuples of two int64 tensors - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
    "\n",
    "At the output we want to get two tensors:\n",
    "- Padded word/token indexes\n",
    "- Padded tag indexes\n",
    "\n",
    "**Exercise. Implement the TransformersCollator class. <font color='red'>(2 points)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BonAp65jQEMD"
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "\n",
    "class TransformersCollator:\n",
    "    \"\"\"\n",
    "    Transformers Collator that handles variable-size sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        tokenizer_kwargs: Dict[str, Any],\n",
    "        label_padding_value: int,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_kwargs = tokenizer_kwargs\n",
    "\n",
    "        self.label_padding_value = label_padding_value\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: List[Tuple[List[str], List[int]]],\n",
    "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        tokens, labels = zip(*batch)\n",
    "\n",
    "        # Use tokenizer to encode tokens\n",
    "        encoded_tokens = self.tokenizer(\n",
    "            list(tokens),\n",
    "            **self.tokenizer_kwargs\n",
    "        )\n",
    "        \n",
    "        # Encode labels to match tokenizer output\n",
    "        encoded_labels = self.encode_labels(\n",
    "            encoded_tokens,\n",
    "            labels,\n",
    "            self.label_padding_value\n",
    "        )\n",
    "\n",
    "        tokens = encoded_tokens\n",
    "        labels = encoded_labels\n",
    "        \n",
    "        # Remove offset_mapping as it's not needed for model input\n",
    "        tokens.pop(\"offset_mapping\")\n",
    "\n",
    "        return tokens, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_labels(\n",
    "        tokens: BatchEncoding,\n",
    "        labels: List[List[int]],\n",
    "        label_padding_value: int,\n",
    "    ) -> torch.LongTensor:\n",
    "\n",
    "        encoded_labels = []\n",
    "\n",
    "        for doc_labels, doc_offset in zip(labels, tokens.offset_mapping):\n",
    "\n",
    "            doc_enc_labels = np.ones(len(doc_offset), dtype=int) * label_padding_value\n",
    "            arr_offset = np.array(doc_offset)\n",
    "\n",
    "            doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "            encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "        return torch.LongTensor(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iC8JkUPnQEMD"
   },
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"is_split_into_words\":    True,\n",
    "    \"return_offsets_mapping\": True,\n",
    "    \"padding\":                True,\n",
    "    \"truncation\":             True,\n",
    "    \"max_length\":             512,\n",
    "    \"return_tensors\":         \"pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sCDaxR6QEMD"
   },
   "outputs": [],
   "source": [
    "collator = TransformersCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_kwargs=tokenizer_kwargs,\n",
    "    label_padding_value=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eirev0N_QEMD"
   },
   "source": [
    "Now you're ready to define the loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JDrLC6pQEME"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,  # for correct metrics measurements leave batch_size=1\n",
    "    shuffle=False, # for correct metrics measurements leave shuffle=False\n",
    "    collate_fn=collator,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # for correct metrics measurements leave batch_size=1\n",
    "    shuffle=False, # for correct metrics measurements leave shuffle=False\n",
    "    collate_fn=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3zGjEDHQEME"
   },
   "source": [
    "Let's look at what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSWcYEAWQEME"
   },
   "outputs": [],
   "source": [
    "tokens, labels = next(iter(train_dataloader))\n",
    "\n",
    "tokens = tokens.to(device)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTcdU1BlQEME"
   },
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7ZTh97-QEME"
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMprtk9bodM9"
   },
   "outputs": [],
   "source": [
    "train_tokens, train_labels = next(iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "))\n",
    "assert torch.equal(\n",
    "    train_tokens['input_ids'],\n",
    "    torch.tensor([[101, 174, 1358, 22961, 176, 14170, 1840, 1106, 21423, 9304, 10721, 1324, 2495, 12913, 119, 102],\n",
    "                  [101, 11109, 1200, 1602, 6715, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "                )), \"Looks like a bug in the collator\"\n",
    "assert torch.equal(\n",
    "    train_tokens['attention_mask'],\n",
    "    torch.tensor([\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ])), \"Looks like a bug in the collator\"\n",
    "assert torch.equal(\n",
    "    train_labels,\n",
    "    torch.tensor([\n",
    "        [-1, 3, -1, 0, 2, -1, 0, 0, 0, 2, -1, -1, 0, -1, 0, -1],\n",
    "        [-1, 4, -1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "    ])), \"Looks like a bug in the collator\"\n",
    "\n",
    "valid_tokens, valid_labels = next(iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "))\n",
    "assert torch.equal(\n",
    "    valid_tokens['input_ids'],\n",
    "    torch.tensor([\n",
    "        [101, 5428, 118, 5837, 18117, 5759, 15189, 1321, 1166, 1120, 1499, 1170, 6687, 2681, 119, 102],\n",
    "        [101, 25338, 17996, 1820, 118, 4775, 118, 1476, 102, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ])), \"Looks like a bug in the collator\"\n",
    "assert torch.equal(\n",
    "    valid_tokens['attention_mask'],\n",
    "    torch.tensor([\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ])), \"Looks like a bug in the collator\"\n",
    "assert torch.equal(\n",
    "    valid_labels,\n",
    "    torch.tensor([\n",
    "        [-1,  0,  0,  3, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "        [-1,  1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "    ])), \"Looks like a bug in the collator\"\n",
    "\n",
    "test_tokens, test_labels = next(iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "))\n",
    "assert torch.equal(\n",
    "    test_tokens['input_ids'],\n",
    "    torch.tensor([\n",
    "        [101, 5862, 118, 179, 26519, 1179, 1243, 6918, 1782, 117, 5144, 1161, 1107, 3774, 3326, 119, 102],\n",
    "        [101, 9468, 3309, 1306, 19122, 2293, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ])), \"Looks like a bug in the collator\"\n",
    "assert torch.equal(\n",
    "    test_tokens['attention_mask'],\n",
    "    torch.tensor([\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ])), \"Looks like a bug in the collator\"\n",
    "assert torch.equal(\n",
    "    test_labels,\n",
    "    torch.tensor([\n",
    "        [-1,  0,  0,  1, -1, -1,  0,  0,  0,  0,  4, -1,  0,  0,  0,  0, -1],\n",
    "        [-1,  4, -1, -1,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "    ])), \"Looks like a bug in the collator\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m-taH0SQEMF"
   },
   "source": [
    "The **transformers** library contains classes for the BERT model, already customized to solve specific problems, with corresponding classification heads. For the NER task we will use the `BertForTokenClassification` class.\n",
    "\n",
    "By analogy with tokenizers, we can use the `AutoModelForTokenClassification` class, which, based on the name of the model, will determine which class is needed to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6tq_i7JQEMF"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vma9yj0zQEMF"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2idx),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Imv-6gAQQEMG"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAdHfn4oQEMG"
   },
   "outputs": [],
   "source": [
    "outputs = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-kTke_8QEMG"
   },
   "outputs": [],
   "source": [
    "assert 2 < criterion(outputs[\"logits\"].transpose(1, 2), labels) < 3\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Ana4qGKeHrN"
   },
   "outputs": [],
   "source": [
    "# let's create a SummaryWriter for experimenting with BiLSTMModel\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"logs/Transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sNuFPRdQEMH"
   },
   "source": [
    "### Experiments\n",
    "\n",
    "Run experiments on the data. Adjust parameters based on the validation set without using the test set. Your goal is to configure the network so that the quality of the model according to the F1-macro measure on the validation and test sets is no less than **0.9**.\n",
    "\n",
    "Draw conclusions about model quality, overfitting, and sensitivity of the architecture to the choice of hyperparameters. Present the results of your experiments in the form of a mini-report (in the same ipython notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IfkN20lrN0J"
   },
   "source": [
    "You can use the same train function as before, except that instead of `model(tokens)` inference you need to do `model(**tokens)`, and instead of `outputs` you use `outputs[\"logits\"].transpose(1, 2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iyZUFddzYE5"
   },
   "source": [
    "**Exercise. Conduct experiments.** **<font color='red'>(2 points)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iv92wK5P7pjl"
   },
   "outputs": [],
   "source": [
    "# Start a new wandb run for Transformer experiment\n",
    "wandb.finish()  # End previous run\n",
    "wandb.init(\n",
    "    project=\"ner-homework\",\n",
    "    name=\"transformer-experiment\",\n",
    "    config={\n",
    "        \"model_type\": \"DistilBERT\",\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 3,\n",
    "        \"max_length\": 512\n",
    "    }\n",
    ")\n",
    "\n",
    "# Experiment with Transformer model\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2idx),\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "# Create datasets for transformers\n",
    "train_dataset = TransformersDataset(\n",
    "    token_seq=train_token_seq,\n",
    "    label_seq=train_label_seq,\n",
    ")\n",
    "valid_dataset = TransformersDataset(\n",
    "    token_seq=valid_token_seq,\n",
    "    label_seq=valid_label_seq,\n",
    ")\n",
    "test_dataset = TransformersDataset(\n",
    "    token_seq=test_token_seq,\n",
    "    label_seq=test_label_seq,\n",
    ")\n",
    "\n",
    "# Create collator\n",
    "tokenizer_kwargs = {\n",
    "    \"is_split_into_words\":    True,\n",
    "    \"return_offsets_mapping\": True,\n",
    "    \"padding\":                True,\n",
    "    \"truncation\":             True,\n",
    "    \"max_length\":             512,\n",
    "    \"return_tensors\":         \"pt\",\n",
    "}\n",
    "\n",
    "collator = TransformersCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_kwargs=tokenizer_kwargs,\n",
    "    label_padding_value=-1,\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "# Modified training functions for transformer\n",
    "def train_epoch_transformer(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    One training cycle (loop) for transformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    for i, (tokens, labels) in tqdm(\n",
    "        enumerate(dataloader),\n",
    "        total=len(dataloader),\n",
    "        desc=\"loop over train batches\",\n",
    "    ):\n",
    "\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Loss calculation and optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(**tokens)\n",
    "        loss = criterion(outputs[\"logits\"].transpose(1, 2), labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        # Log to wandb every 50 batches\n",
    "        if i % 50 == 0:\n",
    "            wandb.log({\n",
    "                \"train/batch_loss\": loss.item(),\n",
    "                \"train/epoch\": epoch,\n",
    "                \"train/batch\": epoch * len(dataloader) + i\n",
    "            })\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs_inference = model(**tokens)\n",
    "            model.train()\n",
    "\n",
    "        batch_metrics = compute_metrics(\n",
    "            outputs=outputs_inference[\"logits\"].transpose(1, 2),\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        for metric_name, metric_value in batch_metrics.items():\n",
    "            batch_metrics_list[metric_name].append(metric_value)\n",
    "\n",
    "    avg_loss = np.mean(epoch_loss)\n",
    "    print(f\"Train loss: {avg_loss}\\n\")\n",
    "    \n",
    "    # Log epoch metrics to wandb\n",
    "    wandb_log = {\"train/loss\": avg_loss, \"epoch\": epoch}\n",
    "\n",
    "    for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "        metric_value = np.mean(metric_value_list)\n",
    "        print(f\"Train {metric_name}: {metric_value}\\n\")\n",
    "        wandb_log[f\"train/{metric_name}\"] = metric_value\n",
    "    \n",
    "    wandb.log(wandb_log)\n",
    "\n",
    "def evaluate_epoch_transformer(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    split_name: str = \"val\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    One evaluation cycle (loop) for transformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (tokens, labels) in tqdm(\n",
    "            enumerate(dataloader),\n",
    "            total=len(dataloader),\n",
    "            desc=f\"loop over {split_name} batches\",\n",
    "        ):\n",
    "\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Loss calculation\n",
    "            outputs = model(**tokens)\n",
    "            loss = criterion(outputs[\"logits\"].transpose(1, 2), labels)\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            batch_metrics = compute_metrics(\n",
    "                outputs=outputs[\"logits\"].transpose(1, 2),\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            for metric_name, metric_value in batch_metrics.items():\n",
    "                batch_metrics_list[metric_name].append(metric_value)\n",
    "\n",
    "        avg_loss = np.mean(epoch_loss)\n",
    "        print(f\"{split_name.capitalize()} loss:  {avg_loss}\\n\")\n",
    "        \n",
    "        # Log epoch metrics to wandb\n",
    "        wandb_log = {f\"{split_name}/loss\": avg_loss, \"epoch\": epoch}\n",
    "\n",
    "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "            metric_value = np.mean(metric_value_list)\n",
    "            print(f\"{split_name.capitalize()} {metric_name}: {metric_value}\\n\")\n",
    "            wandb_log[f\"{split_name}/{metric_name}\"] = metric_value\n",
    "        \n",
    "        wandb.log(wandb_log)\n",
    "\n",
    "def train_transformer(\n",
    "    n_epochs: int,\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Training loop for transformer.\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
    "\n",
    "        train_epoch_transformer(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "        evaluate_epoch_transformer(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "            split_name=\"val\",\n",
    "        )\n",
    "\n",
    "print(\"Starting Transformer experiment...\")\n",
    "print(f\"Model: {model_name}, lr=5e-5, batch_size=8\")\n",
    "\n",
    "train_transformer(\n",
    "    n_epochs=3,\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=valid_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Test final model on test set\n",
    "print(\"Final evaluation on test set:\")\n",
    "evaluate_epoch_transformer(\n",
    "    model=model,\n",
    "    dataloader=test_dataloader,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    epoch=0,\n",
    "    split_name=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XlI3cb1QEL2"
   },
   "source": [
    "## Part 4 - Bonus. BiLSTMAttention-tagger (2 points)\n",
    "\n",
    "You need to carry out the same experiments as in part 2, but using the improved BiLSTM tagger architecture with the Attention mechanism.\n",
    "\n",
    "**Please note** that you do not need to implement Attention yourself; you can use `torch.nn.MultiheadAttention`.\n",
    "\n",
    "Also draw conclusions about model quality, overfitting, sensitivity of the architecture to the choice of hyperparameters, and do a little comparative analysis with the previous architecture. Present the results of your experiments in the form of a mini-report (in the same ipython notebook).\n",
    "\n",
    "**Exercise. Implement the model class BiLSTMAttn.** **<font color='red'>(1 point)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MyLQp047yID"
   },
   "outputs": [],
   "source": [
    "class BiLSTMAttn(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM with Attention architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        bidirectional: bool,\n",
    "        n_classes: int,\n",
    "        num_attention_heads: int = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Calculate input size for attention\n",
    "        rnn_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Multi-head attention layer\n",
    "        self.attention = torch.nn.MultiheadAttention(\n",
    "            embed_dim=rnn_output_size,\n",
    "            num_heads=num_attention_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = torch.nn.LayerNorm(rnn_output_size)\n",
    "        \n",
    "        self.head = torch.nn.Linear(rnn_output_size, n_classes)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tokens: torch.LongTensor) -> torch.Tensor:\n",
    "        embed = self.embedding(tokens)\n",
    "\n",
    "        # we use the special function pack_padded_sequence in order to obtain a PackedSequence structure\n",
    "        # that does not take padding into account when passing rnn\n",
    "        length = (tokens != 0).sum(dim=1).detach().cpu()\n",
    "        packed_embed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embed, length, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # we use the special function pad_packed_sequence to get a tensor from PackedSequence\n",
    "        packed_rnn_output, _ = self.rnn(packed_embed)\n",
    "        rnn_output, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_rnn_output, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Create attention mask to ignore padding tokens\n",
    "        attention_mask = (tokens == 0)  # True for padding tokens\n",
    "        \n",
    "        # Apply self-attention\n",
    "        attn_output, _ = self.attention(\n",
    "            query=rnn_output,\n",
    "            key=rnn_output,\n",
    "            value=rnn_output,\n",
    "            key_padding_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        output = self.layer_norm(rnn_output + attn_output)\n",
    "        \n",
    "        # Apply dropout\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        logits = self.head(output)\n",
    "        return logits.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezh9kLTkQEL9"
   },
   "source": [
    "**Exercise. Conduct experiments and beat the metric value from part 2.** **<font color='red'>(1 point)</font>**\n",
    "\n",
    "P.S. If quality didn't increase, this needs to be justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sE1C1tzEQEL-"
   },
   "outputs": [],
   "source": [
    "# Start a new wandb run for BiLSTM + Attention experiment\n",
    "wandb.finish()  # End previous run\n",
    "wandb.init(\n",
    "    project=\"ner-homework\",\n",
    "    name=\"bilstm-attention-experiment\",\n",
    "    config={\n",
    "        \"model_type\": \"BiLSTM+Attention\",\n",
    "        \"embedding_dim\": 100,\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"bidirectional\": True,\n",
    "        \"attention_heads\": 8,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 5\n",
    "    }\n",
    ")\n",
    "\n",
    "# Experiment with BiLSTM + Attention\n",
    "model_attn = BiLSTMAttn(\n",
    "    num_embeddings=len(token2idx),\n",
    "    embedding_dim=100,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    bidirectional=True,\n",
    "    n_classes=len(label2idx),\n",
    "    num_attention_heads=8,\n",
    ").to(device)\n",
    "\n",
    "optimizer_attn = torch.optim.Adam(model_attn.parameters(), lr=1e-3)\n",
    "criterion_attn = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "# Recreate original dataloaders for BiLSTM experiments\n",
    "train_dataset_lstm = NERDataset(\n",
    "    token_seq=train_token_seq,\n",
    "    label_seq=train_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "valid_dataset_lstm = NERDataset(\n",
    "    token_seq=valid_token_seq,\n",
    "    label_seq=valid_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "test_dataset_lstm = NERDataset(\n",
    "    token_seq=test_token_seq,\n",
    "    label_seq=test_label_seq,\n",
    "    token2idx=token2idx,\n",
    "    label2idx=label2idx,\n",
    ")\n",
    "\n",
    "collator_lstm = NERCollator(\n",
    "    token_padding_value=token2idx[\"<PAD>\"],\n",
    "    label_padding_value=-1,\n",
    ")\n",
    "\n",
    "train_dataloader_lstm = torch.utils.data.DataLoader(\n",
    "    train_dataset_lstm,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_lstm,\n",
    ")\n",
    "valid_dataloader_lstm = torch.utils.data.DataLoader(\n",
    "    valid_dataset_lstm,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator_lstm,\n",
    ")\n",
    "test_dataloader_lstm = torch.utils.data.DataLoader(\n",
    "    test_dataset_lstm,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator_lstm,\n",
    ")\n",
    "\n",
    "print(\"Starting BiLSTM + Attention experiment...\")\n",
    "print(f\"Model parameters: embedding_dim=100, hidden_size=128, num_layers=2, dropout=0.1, lr=1e-3, attention_heads=8\")\n",
    "\n",
    "train(\n",
    "    n_epochs=5,\n",
    "    model=model_attn,\n",
    "    train_dataloader=train_dataloader_lstm,\n",
    "    test_dataloader=valid_dataloader_lstm,\n",
    "    optimizer=optimizer_attn,\n",
    "    criterion=criterion_attn,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Test final model on test set\n",
    "print(\"Final evaluation on test set (BiLSTM + Attention):\")\n",
    "evaluate_epoch(\n",
    "    model=model_attn,\n",
    "    dataloader=test_dataloader_lstm,\n",
    "    criterion=criterion_attn,\n",
    "    device=device,\n",
    "    epoch=0,\n",
    "    split_name=\"test\",\n",
    ")\n",
    "\n",
    "# Final cleanup\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\n=== EXPERIMENT SUMMARY ===\")\n",
    "print(\"1. BiLSTM: Basic bidirectional LSTM with dropout\")\n",
    "print(\"2. Transformer (DistilBERT): Pre-trained transformer model fine-tuned for NER\")\n",
    "print(\"3. BiLSTM + Attention: BiLSTM enhanced with multi-head self-attention\")\n",
    "print(\"\\nExpected results:\")\n",
    "print(\"- Transformer should achieve highest F1-macro (>0.9)\")\n",
    "print(\"- BiLSTM + Attention should outperform basic BiLSTM\") \n",
    "print(\"- Basic BiLSTM should achieve F1-macro around 0.76\")\n",
    "print(\"\\nAll experiments have been logged to Weights & Biases.\")\n",
    "print(\"Check your wandb dashboard for detailed metrics and visualizations!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4MIrbmNoQEMA"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "hrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
